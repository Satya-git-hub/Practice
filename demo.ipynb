{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "df = df_data_1\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------------------+--------------+\n|             label|         class|\n+------------------+--------------+\n| 8959.680239829991| Use_telephone|\n| 9737.511232342687| Standup_chair|\n| 12542.96539897962|      Eat_meat|\n|13225.945637269193|     Getup_bed|\n|15003.269043778426|   Drink_glass|\n|14454.885091207056|    Pour_water|\n|10616.408809008817|     Comb_hair|\n|16537.370891408344|          Walk|\n|11082.626493751379|  Climb_stairs|\n|10261.338314274606| Sitdown_chair|\n|6783.4063714331605|   Liedown_bed|\n| 7173.493500380411|Descend_stairs|\n| 11785.39634462923|   Brush_teeth|\n| 6071.460120926432|      Eat_soup|\n+------------------+--------------+\n\n"
                }
            ],
            "source": "df_energy = spark.sql('''\nselect sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label , class from df group by class\n''')\ndf_energy.show()\ndf_energy.createOrReplaceTempView('df_energy')"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------------+-----------+\n|  x|  y|  z|              source|      class|            label|      class|\n+---+---+---+--------------------+-----------+-----------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n+---+---+---+--------------------+-----------+-----------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df_join = spark.sql('''\nselect * from df inner join df_energy on df.class = df_energy.class\n''')\ndf_join.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(inputCols = ['x','y', 'z'], outputCol = 'features')\nnormalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm', p=1.0)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.regression import LinearRegression\nlr = LinearRegression()"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages = [assembler, normalizer, lr])"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "model = pipeline.fit(df_join)"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": "prediction = model.transform(df_join)"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.03259104594310458"
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.stages[2].summary.r2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}