# Data Preprocessing 

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('D:/IT DOCxxx/datasets/Hitachi Data Engineer Hiring Challenge/Train.csv')

data.columns #check all column names 
data.info() #check no of rows and col type in the dataset 
data.dtypes #check the datatype of all the columns 
data.select_dtypes(include=['object']) #check all teh cat cols 
data.current_day.nunique() #check no of labels in the cols [current_day]


# 1> Dropping cols with high cardinality 
max_cardinality = 100
drop_cols = [
             columns for columns in data.select_dtypes(include=['object'])
             if data[columns].nunique() > max_cardinality
             ]
data = data.drop(drop_cols, axis=1)


# 2> Dropping irrelevant columns
irrelevant_cols = ['current_date', 'target', 'current_year']
data = data.drop(irrelevant_cols, axis=1)


# 3> Dropping duplicate rows
dup_rows = data[data.isnull().any(axis=1)] #store all the duplicate rows
data = data.duplicated()

# Selecting duplicate rows except first occurrence based on all columns
duplicateRows = data[data.duplicated()]
duplicateRows.info()
data.drop_duplicates(keep=False,inplace=True) 


# 4> Imputing null values 
cat_data = data.select_dtypes(include=['object', 'bool'])
cont_data = data.select_dtypes(exclude=['object', 'bool'])


CASE-1
``````
#Works well with numerical features
data['Age'] = data.Age.fillna(data.Age.median())
###

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values='nan', strategy='mean')
cont_data = imputer.fit_transform(cont_data)
###

import sys
from impyute.imputation.cs import fast_knn
sys.setrecursionlimit(100000) #Increase the recursion limit of the OS

# start the KNN training
missing_cont=fast_knn(cont_data.values, k=30) #returns a numpy.ndarray
cont_data.info()
test_dict = {}
for i in range(0,len(list(cont_data.columns))):
        test_dict.update({list(cont_data.columns)[i] : missing_cont[:,i]})
cont_data = pd.DataFrame(test_dict)
cont_data.info()

CASE-2
``````
#Works well with categorical features.

from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer( strategy='most_frequent')
imp_mean.fit(cat_data)
missing_cat = imp_mean.transform(cat_data)

cat_data[list(cat_data.columns)] = missing_cat
cat_data.info()


# 5> Removing outliers

# 5> Feature Scaling 

test_matrix = np.array([
                [7,5,4],
                [6,5,60],
                [10,23,14]
                ])

'''
Normalization rescales the values into a range of [0,1]. 
This might be useful in some cases where all parameters need to have the same positive scale. 
However, the outliers from the data set are lost.
'''
#x - x(min) / x(max ) - x(min)
from sklearn.preprocessing import Normalizer
normalizer = Normalizer()
test_matrix = normalizer.fit_transform(test_matrix)

'''
Standardization rescales data to have a mean (μ) of 0 and standard deviation (σ) of 1 
'''
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler

#to scale the features between a range like (0,1)
min_max_scaler = MinMaxScaler(feature_range=(0,1))
test_matrix = min_max_scaler.fit_transform(test_matrix)

#to scale in the standard way 
# x-mean / std
standard_scaler = StandardScaler()
test_matrix = standard_scaler.fit_transform(test_matrix)

from sklearn.preprocessing import StandardScaler
scale = StandardScaler()
scaled_data = scale.fit_transform(cont_data)

test_dict = {}
for i in range(0,len(list(cont_data.columns))):
        test_dict.update({list(cont_data.columns)[i] : scaled_data[:,i]})
cont_data = pd.DataFrame(test_dict)
cont_data.info()



#to sclae in a way that the data lies between [-1,1]
max_abs_scaler = MaxAbsScaler()
test_matrix = max_abs_scaler.fit_transform(test_matrix)

#scaling data with outliers
from sklearn.preprocessing import RobustScaler
rob_scaler = RobustScaler(with_centering=True, with_scaling=True,
                          quantile_range=(25.0, 75.0)
                          )

test_matrix=rob_scaler.fit_transform(test_matrix)



# 6> Handling categorical features 

#https://medium.com/@guaisang/handling-categorical-features-get-dummies-onehotencoder-and-multicollinearity-f9d473a40417

CASE-1:
```````
dataset = pd.get_dummies(all_data,drop_first=True)

CASE-2:
```````
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(categories=[genders, locations, browsers])

#If there is a possibility that the training data might have missing categorical features, it can often be better to specify handle_unknown='ignore'


'''
LabelEncoder and OneHotEncoder is usually need to be used together as a two steps method to encode categorical features. 
LabelEncoder outputs a dataframe type while OneHotEncoder outputs a numpy array. 
OneHotEncoder has the option to output a sparse matrix. 
DictVectorizer is a one step method to encode and support sparse matrix output. 
Pandas get dummies method is so far the most straight forward and easiest way to encode categorical features. 
The output will remain dataframe type.

The first choice method will be pandas get dummies. 
But if the number of categorical features are huge, DictVectorizer will be a good choice as it supports sparse matrix output.
'''





all_data = pd.concat([cont_data,cat_data], axis=1)





#EXTRAAAS
'''
you want to bin the numerical data, because you have a range of ages and fares. 
However, there might be fluctuations in those numbers that don't reflect patterns in the data, which might be noise. 
That's why you'll put people that are within a certain range of age or fare in the same bin. 
You can do this by using the pandas function qcut() to bin your numerical data:
'''
# Binning numerical columns
data['CatAge'] = pd.qcut(data.Age, q=4, labels=False )
data['CatFare']= pd.qcut(data.Fare, q=4, labels=False)
